{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Gradient in TF\n",
    "## Tensors & Variables in TF for Gradient Descent\n",
    "\n",
    "M. Amintoosi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\.conda\\envs\\p310\\lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.12.2 when it was built against 1.12.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "تفاوت Tensor و Variable:\n",
    "\n",
    "1. tf.Variable: معمولاً برای نگهداری وزن‌ها و بایاس‌ها در شبکه‌های عصبی استفاده می‌شود. یک متغیر قابل تغییر است ولی با استفاده از عملگرهای خاص به روزرسانی میشود. \n",
    "\n",
    "2. tf.Tensor: معمولاً به عنوان ورودی یا خروجی به لایه‌ها و عملگرها مورد استفاده قرار می‌گیرد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto gradient using Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n",
      "tf.Tensor(3.0, shape=(), dtype=float32) tf.Tensor(2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(0.0)\n",
    "print(x)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = 2 * x + 3\n",
    "grad_of_y_wrt_x = tape.gradient(y, x)\n",
    "print(y, grad_of_y_wrt_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto gradient using Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.convert_to_tensor(0.0)\n",
    "print(x)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = 2 * x + 3\n",
    "grad_of_y_wrt_x = tape.gradient(y, x)\n",
    "print(x,grad_of_y_wrt_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تبدیل مقادیر عددی معمولی  به تنسور"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=3.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=-3.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.5>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 3.0\n",
    "y = -x\n",
    "w = 0.5\n",
    "x, y, w = [tf.convert_to_tensor(float(a)) for a in [x, y, w]]\n",
    "x, y, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "## الگوریتم گرادیان کاهشی با تنسورها\n",
    "\n",
    "در مدل زیر \n",
    "$x$\n",
    "و\n",
    "$y$\n",
    "معلوم هستند، هدف پیدا کردن  ضریب \n",
    "$x$\n",
    "هست:\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\Large y = -x \n",
    "$$\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "\n",
    "به فرض\n",
    "$x\\times w$\n",
    "خروجی نورون هست،‌ به دنبال \n",
    "$w$ی بهینه هستیم\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 20.25,  w = 00.23 \n",
      "Loss = 13.62,  w = 00.01 \n",
      "Loss = 09.16,  w = -0.17 \n",
      "Loss = 06.16,  w = -0.32 \n",
      "Loss = 04.14,  w = -0.44 \n",
      "Loss = 02.78,  w = -0.54 \n",
      "Loss = 01.87,  w = -0.63 \n",
      "Loss = 01.26,  w = -0.69 \n",
      "Loss = 00.85,  w = -0.75 \n",
      "Loss = 00.57,  w = -0.79 \n",
      "Loss = 00.38,  w = -0.83 \n",
      "Loss = 00.26,  w = -0.86 \n",
      "Loss = 00.17,  w = -0.89 \n",
      "Loss = 00.12,  w = -0.91 \n",
      "Loss = 00.08,  w = -0.92 \n",
      "Loss = 00.05,  w = -0.94 \n",
      "Loss = 00.04,  w = -0.95 \n",
      "Loss = 00.02,  w = -0.96 \n",
      "Loss = 00.02,  w = -0.97 \n",
      "Loss = 00.01,  w = -0.97 \n"
     ]
    }
   ],
   "source": [
    "x = tf.convert_to_tensor(3.0)\n",
    "w = tf.convert_to_tensor(0.5)\n",
    "y = tf.convert_to_tensor(-3.0)\n",
    "for _ in range(20):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(w)\n",
    "        loss = (y - w * x) ** 2\n",
    "    grad_loss = tape.gradient(loss, w)\n",
    "    w -= 0.01 * grad_loss\n",
    "    print(\"Loss = {:05.2f},  w = {:05.2f} \".format(loss.numpy(), w.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "مشتق‌گیری برحسب ایکس به جای دبلیو"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 20.25,  x = 02.55 \n",
      "Loss = 18.28,  x = 02.12 \n",
      "Loss = 16.49,  x = 01.72 \n",
      "Loss = 14.89,  x = 01.33 \n",
      "Loss = 13.43,  x = 00.96 \n",
      "Loss = 12.12,  x = 00.62 \n",
      "Loss = 10.94,  x = 00.29 \n",
      "Loss = 09.88,  x = -0.03 \n",
      "Loss = 08.91,  x = -0.33 \n",
      "Loss = 08.04,  x = -0.61 \n",
      "Loss = 07.26,  x = -0.88 \n",
      "Loss = 06.55,  x = -1.14 \n",
      "Loss = 05.91,  x = -1.38 \n",
      "Loss = 05.34,  x = -1.61 \n",
      "Loss = 04.82,  x = -1.83 \n",
      "Loss = 04.35,  x = -2.04 \n",
      "Loss = 03.92,  x = -2.24 \n",
      "Loss = 03.54,  x = -2.43 \n",
      "Loss = 03.20,  x = -2.60 \n",
      "Loss = 02.88,  x = -2.77 \n",
      "Loss = 02.60,  x = -2.93 \n",
      "Loss = 02.35,  x = -3.09 \n",
      "Loss = 02.12,  x = -3.23 \n",
      "Loss = 01.91,  x = -3.37 \n",
      "Loss = 01.73,  x = -3.50 \n",
      "Loss = 01.56,  x = -3.63 \n",
      "Loss = 01.41,  x = -3.75 \n",
      "Loss = 01.27,  x = -3.86 \n",
      "Loss = 01.15,  x = -3.97 \n",
      "Loss = 01.03,  x = -4.07 \n",
      "Loss = 00.93,  x = -4.16 \n",
      "Loss = 00.84,  x = -4.26 \n",
      "Loss = 00.76,  x = -4.34 \n",
      "Loss = 00.69,  x = -4.43 \n",
      "Loss = 00.62,  x = -4.51 \n",
      "Loss = 00.56,  x = -4.58 \n",
      "Loss = 00.50,  x = -4.65 \n",
      "Loss = 00.45,  x = -4.72 \n",
      "Loss = 00.41,  x = -4.78 \n",
      "Loss = 00.37,  x = -4.84 \n",
      "Loss = 00.33,  x = -4.90 \n",
      "Loss = 00.30,  x = -4.96 \n",
      "Loss = 00.27,  x = -5.01 \n",
      "Loss = 00.25,  x = -5.06 \n",
      "Loss = 00.22,  x = -5.11 \n",
      "Loss = 00.20,  x = -5.15 \n",
      "Loss = 00.18,  x = -5.19 \n",
      "Loss = 00.16,  x = -5.23 \n",
      "Loss = 00.15,  x = -5.27 \n",
      "Loss = 00.13,  x = -5.31 \n",
      "Loss = 00.12,  x = -5.34 \n",
      "Loss = 00.11,  x = -5.38 \n",
      "Loss = 00.10,  x = -5.41 \n",
      "Loss = 00.09,  x = -5.44 \n",
      "Loss = 00.08,  x = -5.46 \n",
      "Loss = 00.07,  x = -5.49 \n",
      "Loss = 00.06,  x = -5.52 \n",
      "Loss = 00.06,  x = -5.54 \n",
      "Loss = 00.05,  x = -5.56 \n",
      "Loss = 00.05,  x = -5.59 \n",
      "Loss = 00.04,  x = -5.61 \n",
      "Loss = 00.04,  x = -5.63 \n",
      "Loss = 00.04,  x = -5.64 \n",
      "Loss = 00.03,  x = -5.66 \n",
      "Loss = 00.03,  x = -5.68 \n",
      "Loss = 00.03,  x = -5.70 \n",
      "Loss = 00.02,  x = -5.71 \n",
      "Loss = 00.02,  x = -5.72 \n",
      "Loss = 00.02,  x = -5.74 \n",
      "Loss = 00.02,  x = -5.75 \n",
      "Loss = 00.02,  x = -5.76 \n",
      "Loss = 00.01,  x = -5.78 \n",
      "Loss = 00.01,  x = -5.79 \n",
      "Loss = 00.01,  x = -5.80 \n",
      "Loss = 00.01,  x = -5.81 \n",
      "Loss = 00.01,  x = -5.82 \n",
      "Loss = 00.01,  x = -5.83 \n",
      "Loss = 00.01,  x = -5.84 \n",
      "Loss = 00.01,  x = -5.84 \n",
      "Loss = 00.01,  x = -5.85 \n",
      "Loss = 00.01,  x = -5.86 \n",
      "Loss = 00.00,  x = -5.87 \n",
      "Loss = 00.00,  x = -5.87 \n",
      "Loss = 00.00,  x = -5.88 \n",
      "Loss = 00.00,  x = -5.88 \n",
      "Loss = 00.00,  x = -5.89 \n",
      "Loss = 00.00,  x = -5.90 \n",
      "Loss = 00.00,  x = -5.90 \n",
      "Loss = 00.00,  x = -5.91 \n",
      "Loss = 00.00,  x = -5.91 \n",
      "Loss = 00.00,  x = -5.92 \n",
      "Loss = 00.00,  x = -5.92 \n",
      "Loss = 00.00,  x = -5.92 \n",
      "Loss = 00.00,  x = -5.93 \n",
      "Loss = 00.00,  x = -5.93 \n",
      "Loss = 00.00,  x = -5.93 \n",
      "Loss = 00.00,  x = -5.94 \n",
      "Loss = 00.00,  x = -5.94 \n",
      "Loss = 00.00,  x = -5.94 \n",
      "Loss = 00.00,  x = -5.95 \n"
     ]
    }
   ],
   "source": [
    "x = tf.convert_to_tensor(3.0)\n",
    "w = tf.convert_to_tensor(0.5)\n",
    "y = tf.convert_to_tensor(-3.0)\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        loss = (y - w * x) ** 2\n",
    "    grad_loss = tape.gradient(loss, x)\n",
    "    x -= 0.1 * grad_loss\n",
    "    print(\"Loss = {:05.2f},  x = {:05.2f} \".format(loss.numpy(), x.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf.Variable\n",
    "\n",
    "Again grad wrt w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 20.25,  w = 00.23 \n",
      "Loss = 13.62,  w = 00.01 \n",
      "Loss = 09.16,  w = -0.17 \n",
      "Loss = 06.16,  w = -0.32 \n",
      "Loss = 04.14,  w = -0.44 \n",
      "Loss = 02.78,  w = -0.54 \n",
      "Loss = 01.87,  w = -0.63 \n",
      "Loss = 01.26,  w = -0.69 \n",
      "Loss = 00.85,  w = -0.75 \n",
      "Loss = 00.57,  w = -0.79 \n",
      "Loss = 00.38,  w = -0.83 \n",
      "Loss = 00.26,  w = -0.86 \n",
      "Loss = 00.17,  w = -0.89 \n",
      "Loss = 00.12,  w = -0.91 \n",
      "Loss = 00.08,  w = -0.92 \n",
      "Loss = 00.05,  w = -0.94 \n",
      "Loss = 00.04,  w = -0.95 \n",
      "Loss = 00.02,  w = -0.96 \n",
      "Loss = 00.02,  w = -0.97 \n",
      "Loss = 00.01,  w = -0.97 \n"
     ]
    }
   ],
   "source": [
    "# x = tf.convert_to_tensor(3.0)\n",
    "# w = tf.Variable(0.5)\n",
    "# y = tf.convert_to_tensor(-3.0)\n",
    "\n",
    "x = tf.constant(3.0)\n",
    "w = tf.Variable(0.5)\n",
    "y = tf.constant(-3.0)\n",
    "for _ in range(20):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        loss = (y - w * x) ** 2\n",
    "    grad_loss = tape.gradient(loss, w)\n",
    "    w.assign_sub(0.01 * grad_loss)\n",
    "    print(\"Loss = {:05.2f},  w = {:05.2f} \".format(loss.numpy(), w.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
