{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Gradient in TF\n",
    "## Tensors & Variables in TF for Gradient Descent\n",
    "\n",
    "M. Amintoosi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "تفاوت Tensor و Variable:\n",
    "\n",
    "1. tf.Tensor: معمولاً به عنوان ورودی یا خروجی به لایه‌ها و عملگرها مورد استفاده قرار می‌گیرد.\n",
    "\n",
    "2. tf.Variable: معمولاً برای نگهداری وزن‌ها و بایاس‌ها در شبکه‌های عصبی استفاده می‌شود. یک متغیر قابل تغییر است ولی با استفاده از عملگرهای خاص به روزرسانی میشود. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto gradient using Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "x =  2.00, y = 13.00, dy_dx = 12.00 \n"
     ]
    }
   ],
   "source": [
    "x = tf.convert_to_tensor(2.0)\n",
    "print(x)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = 3 * x ** 2 + 1\n",
    "grad_of_y_wrt_x = tape.gradient(y, x) # dy_dx\n",
    "print(\"x = {:-5.2f}, y = {:-5.2f}, dy_dx = {:-5.2f} \".format(x.numpy(), y.numpy(), grad_of_y_wrt_x.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto gradient using Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>\n",
      "x =  2.00, y = 13.00, dy_dx = 12.00 \n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "print(x)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = 3 * x ** 2 + 1\n",
    "grad_of_y_wrt_x = tape.gradient(y, x)\n",
    "print(\"x = {:-5.2f}, y = {:-5.2f}, dy_dx = {:-5.2f} \".format(x.numpy(), y.numpy(), grad_of_y_wrt_x.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تبدیل مقادیر عددی معمولی  به تنسور"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=13.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.5>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 2.0\n",
    "y = 3 * x ** 2 + 1\n",
    "w = 0.5\n",
    "x, y, w = [tf.convert_to_tensor(float(a)) for a in [x, y, w]]\n",
    "x, y, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "## الگوریتم گرادیان کاهشی برای پیدا کردن کمینه‌ی تابع\n",
    "\n",
    "هدف: پیدا کردن کمینه تابع \n",
    "زیر در حول و حوش صفر.\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\large y = 3x^2+1\n",
    "$$\n",
    "\n",
    "<div dir = \"rtl\">\n",
    "کافیست از یک نقطه\n",
    " (ایکس)\n",
    " تصادفی شروع و در خلاف جهت مشتق تابع حرکت کنیم\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\begin{aligned}\n",
    "x = x - \\eta \\frac{\\textrm{dy}}{\\textrm{d}x}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  3.00, y = 28.00, dy_dx = 18.00 \n",
      "x =  1.20, y =  5.32, dy_dx =  7.20 \n",
      "x =  0.48, y =  1.69, dy_dx =  2.88 \n",
      "x =  0.19, y =  1.11, dy_dx =  1.15 \n",
      "x =  0.08, y =  1.02, dy_dx =  0.46 \n",
      "x =  0.03, y =  1.00, dy_dx =  0.18 \n",
      "x =  0.01, y =  1.00, dy_dx =  0.07 \n",
      "x =  0.00, y =  1.00, dy_dx =  0.03 \n"
     ]
    }
   ],
   "source": [
    "x = tf.convert_to_tensor(3.0) # Strating Point\n",
    "for _ in range(25):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        y = 3 * x ** 2 + 1\n",
    "    grad_loss = tape.gradient(y, x) # dy_dx\n",
    "    print(\"x = {:-5.2f}, y = {:-5.2f}, dy_dx = {:-5.2f} \".format(x.numpy(), y.numpy(), grad_loss.numpy()))\n",
    "    if y.numpy()<1.0001:\n",
    "        break\n",
    "    x -= 0.1 * grad_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    " ## الگوریتم گرادیان کاهشی برای پیدا کردن ضرايب بهینه معادله\n",
    "\n",
    "در مدل زیر \n",
    "$x$\n",
    "و\n",
    "$y$\n",
    "معلوم هستند، هدف پیدا کردن  ضریب \n",
    "$x$\n",
    "هست:\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\large y = 2x \n",
    "$$\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "\n",
    "به فرض\n",
    "$x\\times w$\n",
    "خروجی مدل ما ( که می‌تواند یک نورون باشد) هست،‌ به دنبال \n",
    "$w$ی بهینه هستیم؛\n",
    "یعنی آنی که خطای زیر رو کمینه کند:\n",
    "\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\large error = wx - y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\begin{aligned}\n",
    "loss &= {error}^2\n",
    "= (w x - y)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<div dir = \"rtl\">\n",
    "کافیست از یک نقطه (دبلیو) تصادفی شروع و در خلاف جهت مشتق تابع ضرر حرکت کنیم\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\begin{aligned}\n",
    "w = w - \\eta \\frac{\\textrm{d}loss}{\\textrm{d}{w}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 20.25,  w =  0.50 \n",
      "Loss = 13.62,  w =  0.77 \n",
      "Loss =  9.16,  w =  0.99 \n",
      "Loss =  6.16,  w =  1.17 \n",
      "Loss =  4.14,  w =  1.32 \n",
      "Loss =  2.78,  w =  1.44 \n",
      "Loss =  1.87,  w =  1.54 \n",
      "Loss =  1.26,  w =  1.63 \n",
      "Loss =  0.85,  w =  1.69 \n",
      "Loss =  0.57,  w =  1.75 \n",
      "Loss =  0.38,  w =  1.79 \n",
      "Loss =  0.26,  w =  1.83 \n",
      "Loss =  0.17,  w =  1.86 \n",
      "Loss =  0.12,  w =  1.89 \n",
      "Loss =  0.08,  w =  1.91 \n",
      "Loss =  0.05,  w =  1.92 \n",
      "Loss =  0.04,  w =  1.94 \n",
      "Loss =  0.02,  w =  1.95 \n",
      "Loss =  0.02,  w =  1.96 \n",
      "Loss =  0.01,  w =  1.97 \n"
     ]
    }
   ],
   "source": [
    "x = tf.convert_to_tensor(3.0)\n",
    "y = tf.convert_to_tensor(2*x)\n",
    "\n",
    "w = tf.convert_to_tensor(0.5) # staring point\n",
    "for _ in range(20):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(w)\n",
    "        loss = (w * x - y) ** 2\n",
    "    grad_loss = tape.gradient(loss, w) # d_loss/d_w\n",
    "    print(\"Loss = {:-5.2f},  w = {:-5.2f} \".format(loss.numpy(), w.numpy()))\n",
    "    w -= 0.01 * grad_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 20.25,  w =  0.50 \n",
      "Loss = 13.62,  w =  0.77 \n",
      "Loss =  9.16,  w =  0.99 \n",
      "Loss =  6.16,  w =  1.17 \n",
      "Loss =  4.14,  w =  1.32 \n",
      "Loss =  2.78,  w =  1.44 \n",
      "Loss =  1.87,  w =  1.54 \n",
      "Loss =  1.26,  w =  1.63 \n",
      "Loss =  0.85,  w =  1.69 \n",
      "Loss =  0.57,  w =  1.75 \n",
      "Loss =  0.38,  w =  1.79 \n",
      "Loss =  0.26,  w =  1.83 \n",
      "Loss =  0.17,  w =  1.86 \n",
      "Loss =  0.12,  w =  1.89 \n",
      "Loss =  0.08,  w =  1.91 \n",
      "Loss =  0.05,  w =  1.92 \n",
      "Loss =  0.04,  w =  1.94 \n",
      "Loss =  0.02,  w =  1.95 \n",
      "Loss =  0.02,  w =  1.96 \n",
      "Loss =  0.01,  w =  1.97 \n"
     ]
    }
   ],
   "source": [
    "# x = tf.convert_to_tensor(3.0)\n",
    "# y = tf.convert_to_tensor(2*x)\n",
    "# w = tf.Variable(0.5)\n",
    "\n",
    "x = tf.constant(3.0)\n",
    "y = tf.constant(2*x)\n",
    "w = tf.Variable(0.5)\n",
    "for _ in range(20):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # tape.watch(w) # <- This is commented\n",
    "        loss = (w * x - y) ** 2\n",
    "    grad_loss = tape.gradient(loss, w)\n",
    "    print(\"Loss = {:-5.2f},  w = {:-5.2f} \".format(loss.numpy(), w.numpy()))\n",
    "    w.assign_sub(0.01 * grad_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# حل مثال کمینه سازی تابع اول با کراس"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 28.00,  x =  3.00 \n",
      "Loss =  5.32,  x =  1.20 \n",
      "Loss =  1.69,  x =  0.48 \n",
      "Loss =  1.11,  x =  0.19 \n",
      "Loss =  1.02,  x =  0.08 \n",
      "Loss =  1.00,  x =  0.03 \n",
      "Loss =  1.00,  x =  0.01 \n",
      "Loss =  1.00,  x =  0.00 \n",
      "Loss =  1.00,  x =  0.00 \n",
      "Loss =  1.00,  x =  0.00 \n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "loss = lambda: 3 * x ** 2 + 1\n",
    "for _ in range(10):\n",
    "    print(\"Loss = {:-5.2f},  x = {:-5.2f} \".format(loss().numpy(), x.numpy()))\n",
    "    sgd.minimize(loss, var_list=[x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "مشتق‌گیری برحسب ایکس به جای دبلیو"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 20.25,  x =  3.00 \n",
      "Loss = 18.28,  x =  3.45 \n",
      "Loss = 16.49,  x =  3.88 \n",
      "Loss = 14.89,  x =  4.28 \n",
      "Loss = 13.43,  x =  4.67 \n",
      "Loss = 12.12,  x =  5.04 \n",
      "Loss = 10.94,  x =  5.38 \n",
      "Loss =  9.88,  x =  5.71 \n",
      "Loss =  8.91,  x =  6.03 \n",
      "Loss =  8.04,  x =  6.33 \n",
      "Loss =  7.26,  x =  6.61 \n",
      "Loss =  6.55,  x =  6.88 \n",
      "Loss =  5.91,  x =  7.14 \n",
      "Loss =  5.34,  x =  7.38 \n",
      "Loss =  4.82,  x =  7.61 \n",
      "Loss =  4.35,  x =  7.83 \n",
      "Loss =  3.92,  x =  8.04 \n",
      "Loss =  3.54,  x =  8.24 \n",
      "Loss =  3.20,  x =  8.43 \n",
      "Loss =  2.88,  x =  8.60 \n",
      "Loss =  2.60,  x =  8.77 \n",
      "Loss =  2.35,  x =  8.93 \n",
      "Loss =  2.12,  x =  9.09 \n",
      "Loss =  1.91,  x =  9.23 \n",
      "Loss =  1.73,  x =  9.37 \n",
      "Loss =  1.56,  x =  9.50 \n",
      "Loss =  1.41,  x =  9.63 \n",
      "Loss =  1.27,  x =  9.75 \n",
      "Loss =  1.15,  x =  9.86 \n",
      "Loss =  1.03,  x =  9.97 \n",
      "Loss =  0.93,  x = 10.07 \n",
      "Loss =  0.84,  x = 10.16 \n",
      "Loss =  0.76,  x = 10.26 \n",
      "Loss =  0.69,  x = 10.34 \n",
      "Loss =  0.62,  x = 10.43 \n",
      "Loss =  0.56,  x = 10.51 \n",
      "Loss =  0.50,  x = 10.58 \n",
      "Loss =  0.45,  x = 10.65 \n",
      "Loss =  0.41,  x = 10.72 \n",
      "Loss =  0.37,  x = 10.78 \n",
      "Loss =  0.33,  x = 10.84 \n",
      "Loss =  0.30,  x = 10.90 \n",
      "Loss =  0.27,  x = 10.96 \n",
      "Loss =  0.25,  x = 11.01 \n",
      "Loss =  0.22,  x = 11.06 \n",
      "Loss =  0.20,  x = 11.11 \n",
      "Loss =  0.18,  x = 11.15 \n",
      "Loss =  0.16,  x = 11.19 \n",
      "Loss =  0.15,  x = 11.23 \n",
      "Loss =  0.13,  x = 11.27 \n",
      "Loss =  0.12,  x = 11.31 \n",
      "Loss =  0.11,  x = 11.34 \n",
      "Loss =  0.10,  x = 11.38 \n",
      "Loss =  0.09,  x = 11.41 \n",
      "Loss =  0.08,  x = 11.44 \n",
      "Loss =  0.07,  x = 11.46 \n",
      "Loss =  0.06,  x = 11.49 \n",
      "Loss =  0.06,  x = 11.52 \n",
      "Loss =  0.05,  x = 11.54 \n",
      "Loss =  0.05,  x = 11.56 \n",
      "Loss =  0.04,  x = 11.59 \n",
      "Loss =  0.04,  x = 11.61 \n",
      "Loss =  0.04,  x = 11.63 \n",
      "Loss =  0.03,  x = 11.64 \n",
      "Loss =  0.03,  x = 11.66 \n",
      "Loss =  0.03,  x = 11.68 \n",
      "Loss =  0.02,  x = 11.70 \n",
      "Loss =  0.02,  x = 11.71 \n",
      "Loss =  0.02,  x = 11.72 \n",
      "Loss =  0.02,  x = 11.74 \n",
      "Loss =  0.02,  x = 11.75 \n",
      "Loss =  0.01,  x = 11.76 \n",
      "Loss =  0.01,  x = 11.78 \n",
      "Loss =  0.01,  x = 11.79 \n",
      "Loss =  0.01,  x = 11.80 \n",
      "Loss =  0.01,  x = 11.81 \n",
      "Loss =  0.01,  x = 11.82 \n",
      "Loss =  0.01,  x = 11.83 \n",
      "Loss =  0.01,  x = 11.84 \n",
      "Loss =  0.01,  x = 11.84 \n",
      "Loss =  0.01,  x = 11.85 \n",
      "Loss =  0.00,  x = 11.86 \n",
      "Loss =  0.00,  x = 11.87 \n",
      "Loss =  0.00,  x = 11.87 \n",
      "Loss =  0.00,  x = 11.88 \n",
      "Loss =  0.00,  x = 11.88 \n",
      "Loss =  0.00,  x = 11.89 \n",
      "Loss =  0.00,  x = 11.90 \n",
      "Loss =  0.00,  x = 11.90 \n",
      "Loss =  0.00,  x = 11.91 \n",
      "Loss =  0.00,  x = 11.91 \n",
      "Loss =  0.00,  x = 11.92 \n",
      "Loss =  0.00,  x = 11.92 \n",
      "Loss =  0.00,  x = 11.92 \n",
      "Loss =  0.00,  x = 11.93 \n",
      "Loss =  0.00,  x = 11.93 \n",
      "Loss =  0.00,  x = 11.93 \n",
      "Loss =  0.00,  x = 11.94 \n",
      "Loss =  0.00,  x = 11.94 \n",
      "Loss =  0.00,  x = 11.94 \n"
     ]
    }
   ],
   "source": [
    "x = tf.convert_to_tensor(3.0)\n",
    "y = tf.convert_to_tensor(2*x)\n",
    "w = tf.convert_to_tensor(0.5)\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        loss = (w * x - y) ** 2\n",
    "    grad_loss = tape.gradient(loss, x)\n",
    "    print(\"Loss = {:-5.2f},  x = {:-5.2f} \".format(loss.numpy(), x.numpy()))\n",
    "    x -= 0.1 * grad_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "کمبنه‌سازی فوق با کمینه‌ساز کراس"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 20.25,  x =  3.00,  y =  6.00,  w =  0.50 \n",
      "Loss = 18.28,  x =  3.45,  y =  6.00,  w =  0.50 \n",
      "Loss = 16.49,  x =  3.88,  y =  6.00,  w =  0.50 \n",
      "Loss = 14.89,  x =  4.28,  y =  6.00,  w =  0.50 \n",
      "Loss = 13.43,  x =  4.67,  y =  6.00,  w =  0.50 \n",
      "Loss = 12.12,  x =  5.04,  y =  6.00,  w =  0.50 \n",
      "Loss = 10.94,  x =  5.38,  y =  6.00,  w =  0.50 \n",
      "Loss =  9.88,  x =  5.71,  y =  6.00,  w =  0.50 \n",
      "Loss =  8.91,  x =  6.03,  y =  6.00,  w =  0.50 \n",
      "Loss =  8.04,  x =  6.33,  y =  6.00,  w =  0.50 \n",
      "Loss =  7.26,  x =  6.61,  y =  6.00,  w =  0.50 \n",
      "Loss =  6.55,  x =  6.88,  y =  6.00,  w =  0.50 \n",
      "Loss =  5.91,  x =  7.14,  y =  6.00,  w =  0.50 \n",
      "Loss =  5.34,  x =  7.38,  y =  6.00,  w =  0.50 \n",
      "Loss =  4.82,  x =  7.61,  y =  6.00,  w =  0.50 \n",
      "Loss =  4.35,  x =  7.83,  y =  6.00,  w =  0.50 \n",
      "Loss =  3.92,  x =  8.04,  y =  6.00,  w =  0.50 \n",
      "Loss =  3.54,  x =  8.24,  y =  6.00,  w =  0.50 \n",
      "Loss =  3.20,  x =  8.43,  y =  6.00,  w =  0.50 \n",
      "Loss =  2.88,  x =  8.60,  y =  6.00,  w =  0.50 \n",
      "Loss =  2.60,  x =  8.77,  y =  6.00,  w =  0.50 \n",
      "Loss =  2.35,  x =  8.93,  y =  6.00,  w =  0.50 \n",
      "Loss =  2.12,  x =  9.09,  y =  6.00,  w =  0.50 \n",
      "Loss =  1.91,  x =  9.23,  y =  6.00,  w =  0.50 \n",
      "Loss =  1.73,  x =  9.37,  y =  6.00,  w =  0.50 \n",
      "Loss =  1.56,  x =  9.50,  y =  6.00,  w =  0.50 \n",
      "Loss =  1.41,  x =  9.63,  y =  6.00,  w =  0.50 \n",
      "Loss =  1.27,  x =  9.75,  y =  6.00,  w =  0.50 \n",
      "Loss =  1.15,  x =  9.86,  y =  6.00,  w =  0.50 \n",
      "Loss =  1.03,  x =  9.97,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.93,  x = 10.07,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.84,  x = 10.16,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.76,  x = 10.26,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.69,  x = 10.34,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.62,  x = 10.43,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.56,  x = 10.51,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.50,  x = 10.58,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.45,  x = 10.65,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.41,  x = 10.72,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.37,  x = 10.78,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.33,  x = 10.84,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.30,  x = 10.90,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.27,  x = 10.96,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.25,  x = 11.01,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.22,  x = 11.06,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.20,  x = 11.11,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.18,  x = 11.15,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.16,  x = 11.19,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.15,  x = 11.23,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.13,  x = 11.27,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.12,  x = 11.31,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.11,  x = 11.34,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.10,  x = 11.38,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.09,  x = 11.41,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.08,  x = 11.44,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.07,  x = 11.46,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.06,  x = 11.49,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.06,  x = 11.52,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.05,  x = 11.54,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.05,  x = 11.56,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.04,  x = 11.59,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.04,  x = 11.61,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.04,  x = 11.63,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.03,  x = 11.64,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.03,  x = 11.66,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.03,  x = 11.68,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.02,  x = 11.70,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.02,  x = 11.71,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.02,  x = 11.72,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.02,  x = 11.74,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.02,  x = 11.75,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.01,  x = 11.76,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.01,  x = 11.78,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.01,  x = 11.79,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.01,  x = 11.80,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.01,  x = 11.81,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.01,  x = 11.82,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.01,  x = 11.83,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.01,  x = 11.84,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.01,  x = 11.84,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.01,  x = 11.85,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.86,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.87,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.87,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.88,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.88,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.89,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.90,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.90,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.91,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.91,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.92,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.92,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.92,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.93,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.93,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.93,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.94,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.94,  y =  6.00,  w =  0.50 \n",
      "Loss =  0.00,  x = 11.94,  y =  6.00,  w =  0.50 \n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "y = tf.convert_to_tensor(2*x)\n",
    "w = tf.convert_to_tensor(0.5)\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "loss = lambda : (w * x - y) ** 2 \n",
    "for _ in range(100):\n",
    "    print(\"Loss = {:-5.2f},  x = {:-5.2f},  y = {:-5.2f},  w = {:-5.2f} \"\\\n",
    "          .format(loss().numpy(), x.numpy(), y.numpy(), w.numpy()))\n",
    "    sgd.minimize(loss, var_list=[x])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
