{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Gradient in TF\n",
    "## Tensors & Variables in TF for Gradient Descent\n",
    "\n",
    "M. Amintoosi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\.conda\\envs\\p310\\lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.12.2 when it was built against 1.12.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "تفاوت Tensor و Variable:\n",
    "\n",
    "1. tf.Variable: معمولاً برای نگهداری وزن‌ها و بایاس‌ها در شبکه‌های عصبی استفاده می‌شود. یک متغیر قابل تغییر است ولی با استفاده از عملگرهای خاص به روزرسانی میشود. \n",
    "\n",
    "2. tf.Tensor: معمولاً به عنوان ورودی یا خروجی به لایه‌ها و عملگرها مورد استفاده قرار می‌گیرد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto gradient using Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n",
      "tf.Tensor(3.0, shape=(), dtype=float32) tf.Tensor(2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(0.0)\n",
    "print(x)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = 2 * x + 3\n",
    "grad_of_y_wrt_x = tape.gradient(y, x)\n",
    "print(y, grad_of_y_wrt_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto gradient using Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.convert_to_tensor(0.0)\n",
    "print(x)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = 2 * x + 3\n",
    "grad_of_y_wrt_x = tape.gradient(y, x)\n",
    "print(x, grad_of_y_wrt_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تبدیل مقادیر عددی معمولی  به تنسور"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=3.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.5>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 3.0\n",
    "y = 2*x\n",
    "w = 0.5\n",
    "x, y, w = [tf.convert_to_tensor(float(a)) for a in [x, y, w]]\n",
    "x, y, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "## الگوریتم گرادیان کاهشی با تنسورها\n",
    "\n",
    "در مدل زیر \n",
    "$x$\n",
    "و\n",
    "$y$\n",
    "معلوم هستند، هدف پیدا کردن  ضریب \n",
    "$x$\n",
    "هست:\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\large y = 2x \n",
    "$$\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "\n",
    "به فرض\n",
    "$x\\times w$\n",
    "خروجی نورون هست،‌ به دنبال \n",
    "$w$ی بهینه هستیم\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large error = wx - y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\begin{aligned}\n",
    "Loss &= {error}^2\n",
    "= (w x - y)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<div dir = \"rtl\">\n",
    "کافیست از یک نقطه تصادفی شروع و در خلاف جهت مشتق تابع ضرر حرکت کنیم\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\begin{aligned}\n",
    "w = w - \\eta \\frac{\\partial{Loss}}{\\partial{w}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 20.25,  w = 00.77 \n",
      "Loss = 13.62,  w = 00.99 \n",
      "Loss = 09.16,  w = 01.17 \n",
      "Loss = 06.16,  w = 01.32 \n",
      "Loss = 04.14,  w = 01.44 \n",
      "Loss = 02.78,  w = 01.54 \n",
      "Loss = 01.87,  w = 01.63 \n",
      "Loss = 01.26,  w = 01.69 \n",
      "Loss = 00.85,  w = 01.75 \n",
      "Loss = 00.57,  w = 01.79 \n",
      "Loss = 00.38,  w = 01.83 \n",
      "Loss = 00.26,  w = 01.86 \n",
      "Loss = 00.17,  w = 01.89 \n",
      "Loss = 00.12,  w = 01.91 \n",
      "Loss = 00.08,  w = 01.92 \n",
      "Loss = 00.05,  w = 01.94 \n",
      "Loss = 00.04,  w = 01.95 \n",
      "Loss = 00.02,  w = 01.96 \n",
      "Loss = 00.02,  w = 01.97 \n",
      "Loss = 00.01,  w = 01.97 \n"
     ]
    }
   ],
   "source": [
    "x = tf.convert_to_tensor(3.0)\n",
    "y = tf.convert_to_tensor(6.0)\n",
    "w = tf.convert_to_tensor(0.5)\n",
    "for _ in range(20):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(w)\n",
    "        loss = (w * x - y) ** 2\n",
    "    grad_loss = tape.gradient(loss, w)\n",
    "    w -= 0.01 * grad_loss\n",
    "    print(\"Loss = {:05.2f},  w = {:05.2f} \".format(loss.numpy(), w.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "مشتق‌گیری برحسب ایکس به جای دبلیو"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 20.25,  x = 03.45 \n",
      "Loss = 18.28,  x = 03.88 \n",
      "Loss = 16.49,  x = 04.28 \n",
      "Loss = 14.89,  x = 04.67 \n",
      "Loss = 13.43,  x = 05.04 \n",
      "Loss = 12.12,  x = 05.38 \n",
      "Loss = 10.94,  x = 05.71 \n",
      "Loss = 09.88,  x = 06.03 \n",
      "Loss = 08.91,  x = 06.33 \n",
      "Loss = 08.04,  x = 06.61 \n",
      "Loss = 07.26,  x = 06.88 \n",
      "Loss = 06.55,  x = 07.14 \n",
      "Loss = 05.91,  x = 07.38 \n",
      "Loss = 05.34,  x = 07.61 \n",
      "Loss = 04.82,  x = 07.83 \n",
      "Loss = 04.35,  x = 08.04 \n",
      "Loss = 03.92,  x = 08.24 \n",
      "Loss = 03.54,  x = 08.43 \n",
      "Loss = 03.20,  x = 08.60 \n",
      "Loss = 02.88,  x = 08.77 \n",
      "Loss = 02.60,  x = 08.93 \n",
      "Loss = 02.35,  x = 09.09 \n",
      "Loss = 02.12,  x = 09.23 \n",
      "Loss = 01.91,  x = 09.37 \n",
      "Loss = 01.73,  x = 09.50 \n",
      "Loss = 01.56,  x = 09.63 \n",
      "Loss = 01.41,  x = 09.75 \n",
      "Loss = 01.27,  x = 09.86 \n",
      "Loss = 01.15,  x = 09.97 \n",
      "Loss = 01.03,  x = 10.07 \n",
      "Loss = 00.93,  x = 10.16 \n",
      "Loss = 00.84,  x = 10.26 \n",
      "Loss = 00.76,  x = 10.34 \n",
      "Loss = 00.69,  x = 10.43 \n",
      "Loss = 00.62,  x = 10.51 \n",
      "Loss = 00.56,  x = 10.58 \n",
      "Loss = 00.50,  x = 10.65 \n",
      "Loss = 00.45,  x = 10.72 \n",
      "Loss = 00.41,  x = 10.78 \n",
      "Loss = 00.37,  x = 10.84 \n",
      "Loss = 00.33,  x = 10.90 \n",
      "Loss = 00.30,  x = 10.96 \n",
      "Loss = 00.27,  x = 11.01 \n",
      "Loss = 00.25,  x = 11.06 \n",
      "Loss = 00.22,  x = 11.11 \n",
      "Loss = 00.20,  x = 11.15 \n",
      "Loss = 00.18,  x = 11.19 \n",
      "Loss = 00.16,  x = 11.23 \n",
      "Loss = 00.15,  x = 11.27 \n",
      "Loss = 00.13,  x = 11.31 \n",
      "Loss = 00.12,  x = 11.34 \n",
      "Loss = 00.11,  x = 11.38 \n",
      "Loss = 00.10,  x = 11.41 \n",
      "Loss = 00.09,  x = 11.44 \n",
      "Loss = 00.08,  x = 11.46 \n",
      "Loss = 00.07,  x = 11.49 \n",
      "Loss = 00.06,  x = 11.52 \n",
      "Loss = 00.06,  x = 11.54 \n",
      "Loss = 00.05,  x = 11.56 \n",
      "Loss = 00.05,  x = 11.59 \n",
      "Loss = 00.04,  x = 11.61 \n",
      "Loss = 00.04,  x = 11.63 \n",
      "Loss = 00.04,  x = 11.64 \n",
      "Loss = 00.03,  x = 11.66 \n",
      "Loss = 00.03,  x = 11.68 \n",
      "Loss = 00.03,  x = 11.70 \n",
      "Loss = 00.02,  x = 11.71 \n",
      "Loss = 00.02,  x = 11.72 \n",
      "Loss = 00.02,  x = 11.74 \n",
      "Loss = 00.02,  x = 11.75 \n",
      "Loss = 00.02,  x = 11.76 \n",
      "Loss = 00.01,  x = 11.78 \n",
      "Loss = 00.01,  x = 11.79 \n",
      "Loss = 00.01,  x = 11.80 \n",
      "Loss = 00.01,  x = 11.81 \n",
      "Loss = 00.01,  x = 11.82 \n",
      "Loss = 00.01,  x = 11.83 \n",
      "Loss = 00.01,  x = 11.84 \n",
      "Loss = 00.01,  x = 11.84 \n",
      "Loss = 00.01,  x = 11.85 \n",
      "Loss = 00.01,  x = 11.86 \n",
      "Loss = 00.00,  x = 11.87 \n",
      "Loss = 00.00,  x = 11.87 \n",
      "Loss = 00.00,  x = 11.88 \n",
      "Loss = 00.00,  x = 11.88 \n",
      "Loss = 00.00,  x = 11.89 \n",
      "Loss = 00.00,  x = 11.90 \n",
      "Loss = 00.00,  x = 11.90 \n",
      "Loss = 00.00,  x = 11.91 \n",
      "Loss = 00.00,  x = 11.91 \n",
      "Loss = 00.00,  x = 11.92 \n",
      "Loss = 00.00,  x = 11.92 \n",
      "Loss = 00.00,  x = 11.92 \n",
      "Loss = 00.00,  x = 11.93 \n",
      "Loss = 00.00,  x = 11.93 \n",
      "Loss = 00.00,  x = 11.93 \n",
      "Loss = 00.00,  x = 11.94 \n",
      "Loss = 00.00,  x = 11.94 \n",
      "Loss = 00.00,  x = 11.94 \n",
      "Loss = 00.00,  x = 11.95 \n"
     ]
    }
   ],
   "source": [
    "x = tf.convert_to_tensor(3.0)\n",
    "y = tf.convert_to_tensor(6.0)\n",
    "w = tf.convert_to_tensor(0.5)\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        loss = (w * x - y) ** 2\n",
    "    grad_loss = tape.gradient(loss, x)\n",
    "    x -= 0.1 * grad_loss\n",
    "    print(\"Loss = {:05.2f},  x = {:05.2f} \".format(loss.numpy(), x.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf.Variable\n",
    "\n",
    "Again grad wrt w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 20.25,  w = 00.77 \n",
      "Loss = 13.62,  w = 00.99 \n",
      "Loss = 09.16,  w = 01.17 \n",
      "Loss = 06.16,  w = 01.32 \n",
      "Loss = 04.14,  w = 01.44 \n",
      "Loss = 02.78,  w = 01.54 \n",
      "Loss = 01.87,  w = 01.63 \n",
      "Loss = 01.26,  w = 01.69 \n",
      "Loss = 00.85,  w = 01.75 \n",
      "Loss = 00.57,  w = 01.79 \n",
      "Loss = 00.38,  w = 01.83 \n",
      "Loss = 00.26,  w = 01.86 \n",
      "Loss = 00.17,  w = 01.89 \n",
      "Loss = 00.12,  w = 01.91 \n",
      "Loss = 00.08,  w = 01.92 \n",
      "Loss = 00.05,  w = 01.94 \n",
      "Loss = 00.04,  w = 01.95 \n",
      "Loss = 00.02,  w = 01.96 \n",
      "Loss = 00.02,  w = 01.97 \n",
      "Loss = 00.01,  w = 01.97 \n"
     ]
    }
   ],
   "source": [
    "# x = tf.convert_to_tensor(3.0)\n",
    "# y = tf.convert_to_tensor(6.0)\n",
    "# w = tf.Variable(0.5)\n",
    "\n",
    "x = tf.constant(3.0)\n",
    "y = tf.constant(6.0)\n",
    "w = tf.Variable(0.5)\n",
    "for _ in range(20):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        loss = (w * x - y) ** 2\n",
    "    grad_loss = tape.gradient(loss, w)\n",
    "    w.assign_sub(0.01 * grad_loss)\n",
    "    print(\"Loss = {:05.2f},  w = {:05.2f} \".format(loss.numpy(), w.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
