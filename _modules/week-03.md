---
title: Gradient Descent and its variants
---

## 1402/07/23

* [Gradient Descent](https://www.d2l.ai/chapter_optimization/gd.html) - until 12.3.3.2. Convergence Analysis
* [Matrix Derivative](https://atmos.washington.edu/~dennis/MatrixCalculus.pdf) - Proposition 5, Proposition 7 & 8

* [What worries me about AI](https://medium.com/@francois.chollet/what-worries-me-about-ai-ed9df072b704)

## 1402/07/26

* [Stochastic Gradient Descent](https://www.d2l.ai/chapter_optimization/sgd.html)

## 1402/07/30

Please read the following papers:
* [The Application of Taylor Expansion in Reducing the Size of Convolutional Neural Networks for Classifying Impressionism and Miniature Style Paintings](https://fumcs.github.io/publications/#TaylorExpansion_in_CNN_prunning99) (کاربرد بسط تیلور در کاهش حجم شبکه های عصبی پیچشی برای طبقه بندی نقاشی های سبک امپرسیونیسم و مینیاتور
)

*[Fully Connected to Fully Convolutional: Road to Yesterday](https://fumcs.github.io/publications/#FC2FC_2022)(تمام متصل به تمام پیچشی: پلی به گذشته)


* [Minibatch Stochastic Gradient Descent](https://www.d2l.ai/chapter_optimization/minibatch-sgd.html)
* [Momentum](https://www.d2l.ai/chapter_optimization/momentum.html)
* [Visualization of Mementum](https://milania.de/blog/Introduction_to_neural_network_optimizers_%5Bpart_1%5D_%E2%80%93_momentum_optimization)


**Further Reading**{: .label .label-yellow }

* [Matrix Calculus, d2l](https://www.d2l.ai/chapter_preliminaries/calculus.html) 
* [Matrix Derivative in Wiki](https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions)
* [d2l package installation on google colab](https://stackoverflow.com/questions/76248695/d2l-package-installation-on-google-colab)
* [Preliminaries](https://d2l.ai/chapter_preliminaries/index.html)
* [Linear Neural Networks for Regression](https://d2l.ai/chapter_linear-regression/index.html)